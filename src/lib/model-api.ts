// ç»Ÿä¸€çš„æ¨¡å‹APIè°ƒç”¨æ¥å£

// å‡è®¾localModelDiscoveryçš„ç±»å‹å®šä¹‰
export interface LocalModel {
  id: string
  name: string
  provider: string
  status: 'online' | 'offline' | 'loading'
  capabilities: string[]
  endpoint?: string
}

// æœ¬åœ°æ¨¡å‹å‘ç°å™¨æ¥å£å®šä¹‰
export interface LocalModelDiscovery {
  getDiscoveredModels(): LocalModel[]
  sendMessage(model: LocalModel, messages: ChatMessage[]): Promise<string>
}

// å®ç°æœ¬åœ°æ¨¡å‹å‘ç°å™¨ï¼ˆæ¨¡æ‹Ÿï¼‰
const localModelDiscovery: LocalModelDiscovery = {
  getDiscoveredModels(): LocalModel[] {
    // å®é™…å®ç°ä¸­åº”è¯¥è¿”å›çœŸå®å‘ç°çš„æœ¬åœ°æ¨¡å‹
    return [
      {
        id: 'local-codegemma',
        name: 'CodeGemma Local',
        provider: 'Google',
        status: 'online',
        capabilities: ['code', 'chat'],
        endpoint: 'http://localhost:1234/v1'
      }
    ]
  },
  
  async sendMessage(model: LocalModel, messages: ChatMessage[]): Promise<string> {
    // å®é™…å®ç°ä¸­åº”è¯¥è°ƒç”¨æœ¬åœ°æ¨¡å‹çš„API
    return `æœ¬åœ°æ¨¡å‹${model.name}å“åº”: å·²å¤„ç†${messages.length}æ¡æ¶ˆæ¯`;
  }
};

export interface ChatMessage {
  role: "system" | "user" | "assistant"
  content: string
}

export interface ModelResponse {
  content: string
  model: string
  usage?: {
    prompt_tokens: number
    completion_tokens: number
    total_tokens: number
  }
}

export class ModelAPI {
  // å‘é€æ¶ˆæ¯åˆ°æŒ‡å®šæ¨¡å‹
  static async sendMessage(
    modelId: string,
    messages: ChatMessage[],
    options?: {
      temperature?: number
      max_tokens?: number
      stream?: boolean
    },
  ): Promise<ModelResponse> {
    // æ£€æŸ¥æ˜¯å¦ä¸ºæœ¬åœ°æ¨¡å‹
    const localModels = localModelDiscovery.getDiscoveredModels()
    const localModel = localModels.find((m: LocalModel) => m.id === modelId)

    if (localModel) {
      return await this.sendToLocalModel(localModel, messages, options)
    } else {
      return await this.sendToCloudModel(modelId, messages, options)
    }
  }

  // å‘é€åˆ°æœ¬åœ°æ¨¡å‹ï¼ˆä¿®å¤ï¼šä½¿ç”¨_optionså‚æ•°ï¼Œæ·»åŠ max_tokensé™åˆ¶é€»è¾‘ï¼‰
  private static async sendToLocalModel(
    model: LocalModel,
    messages: ChatMessage[],
    _options?: {
      temperature?: number
      max_tokens?: number
      stream?: boolean
    },
  ): Promise<ModelResponse> {
    try {
      let content = await localModelDiscovery.sendMessage(model, messages)
      const promptTokens = this.estimateTokens(messages.map((m) => m.content).join(" "))
      
      // ä¿®å¤ï¼šä½¿ç”¨_optionsä¸­çš„max_tokensé™åˆ¶å›å¤é•¿åº¦
      if (_options?.max_tokens) {
        const currentTokens = this.estimateTokens(content)
        if (currentTokens > _options.max_tokens) {
          // æŒ‰tokenæ¯”ä¾‹æˆªæ–­å†…å®¹ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯
          const truncateRatio = _options.max_tokens / currentTokens
          const truncateLength = Math.floor(content.length * truncateRatio)
          content = `${content.slice(0, truncateLength)}...\n\nï¼ˆæ³¨ï¼šå†…å®¹å·²æ ¹æ®max_tokensé™åˆ¶æˆªæ–­ï¼‰`
        }
      }

      const completionTokens = this.estimateTokens(content)
      return {
        content,
        model: model.id,
        usage: {
          prompt_tokens: promptTokens,
          completion_tokens: completionTokens,
          total_tokens: promptTokens + completionTokens,
        },
      }
    } catch (error) {
      throw new Error(`æœ¬åœ°æ¨¡å‹ ${model.name} è°ƒç”¨å¤±è´¥: ${error instanceof Error ? error.message : "æœªçŸ¥é”™è¯¯"}`)
    }
  }

  // å‘é€åˆ°äº‘ç«¯æ¨¡å‹ï¼ˆä¿®å¤ï¼šä½¿ç”¨_optionså‚æ•°ï¼Œæ·»åŠ temperatureå’Œmax_tokensé€»è¾‘ï¼‰
  private static async sendToCloudModel(
    modelId: string,
    messages: ChatMessage[],
    _options?: {
      temperature?: number
      max_tokens?: number
      stream?: boolean
    },
  ): Promise<ModelResponse> {
    // è¿™é‡Œå¯ä»¥é›†æˆçœŸå®çš„äº‘ç«¯API
    // ç›®å‰è¿”å›æ¨¡æ‹Ÿå“åº”
    await new Promise((resolve) => setTimeout(resolve, 1000 + Math.random() * 2000))

    // ä¿®å¤ï¼šä½¿ç”¨_optionsä¸­çš„temperatureè°ƒæ•´å›å¤éšæœºæ€§ï¼ˆ0-1ï¼Œå€¼è¶Šé«˜è¶Šéšæœºï¼‰
    const temperature = _options?.temperature ?? 0.7
    const isRandomMode = temperature > 0.5

    // ç”ŸæˆåŸºç¡€å“åº”ï¼ˆæ ¹æ®éšæœºæ€§è°ƒæ•´å†…å®¹ç»†èŠ‚ï¼‰
    let content = this.generateMockResponse(
      messages[messages.length - 1]?.content || "",
      modelId,
      isRandomMode // ä¼ é€’éšæœºæ€§æ ‡è¯†
    )

    const promptTokens = this.estimateTokens(messages.map((m) => m.content).join(" "))
    
    // ä¿®å¤ï¼šä½¿ç”¨_optionsä¸­çš„max_tokensé™åˆ¶å›å¤é•¿åº¦
    if (_options?.max_tokens) {
      const currentTokens = this.estimateTokens(content)
      if (currentTokens > _options.max_tokens) {
        const truncateRatio = _options.max_tokens / currentTokens
        const truncateLength = Math.floor(content.length * truncateRatio)
        content = `${content.slice(0, truncateLength)}...\n\nï¼ˆæ³¨ï¼šå†…å®¹å·²æ ¹æ®max_tokensé™åˆ¶æˆªæ–­ï¼‰`
      }
    }

    const completionTokens = this.estimateTokens(content)
    return {
      content: content,
      model: modelId,
      usage: {
        prompt_tokens: promptTokens,
        completion_tokens: completionTokens,
        total_tokens: promptTokens + completionTokens,
      },
    }
  }

  // ç”Ÿæˆæ¨¡æ‹Ÿå“åº”ï¼ˆæ–°å¢ï¼šæ”¯æŒéšæœºæ€§å‚æ•°isRandomModeï¼‰
  private static generateMockResponse(input: string, modelId: string, isRandomMode: boolean = false): string {
    const lowerInput = input.toLowerCase()

    // æ ¹æ®ä¸åŒæ¨¡å‹è¿”å›ä¸åŒé£æ ¼çš„å›å¤
    const modelStyles = {
      "gpt-4": "ğŸ§  **GPT-4 æ·±åº¦åˆ†æ**\n\n",
      "gpt-3.5": "âš¡ **GPT-3.5 å¿«é€Ÿå›å¤**\n\n",
      "claude-3": "ğŸ“š **Claude-3 è¯¦ç»†è§£ç­”**\n\n",
      "gemini-pro": "ğŸ” **Gemini Pro å¤šç»´åˆ†æ**\n\n",
      "jimeng-ai": "âœ¨ **å³æ¢¦AI åˆ›æ„å›å¤**\n\n",
    }

    const prefix = modelStyles[modelId as keyof typeof modelStyles] || "ğŸ¤– **AIåŠ©æ‰‹å›å¤**\n\n"

    // æ£€æŸ¥è¨€å¯ä¸‡è±¡åˆ›æ„åŠŸèƒ½
    if (["åˆ›æ„", "è®¾è®¡", "æ–‡æ¡ˆ", "å›¾ç‰‡", "è§†é¢‘", "ç”Ÿæˆ", "åˆ¶ä½œ"].some((keyword) => lowerInput.includes(keyword))) {
      // éšæœºæ¨¡å¼ï¼šæ·»åŠ æ›´å¤šåˆ›æ„ç¤ºä¾‹ï¼›ééšæœºæ¨¡å¼ï¼šä¿æŒåŸºç¡€ç¤ºä¾‹
      const creativeExamples = isRandomMode 
        ? [
            "â€¢ çŸ­è§†é¢‘è„šæœ¬ç”Ÿæˆï¼ˆå«åˆ†é•œæè¿°ï¼‰\n",
            "â€¢ ç¤¾äº¤åª’ä½“é…å›¾è®¾è®¡ï¼ˆæŒ‡å®šå¹³å°é£æ ¼ï¼‰\n",
            "â€¢ å“ç‰ŒSloganåˆ›æ„ï¼ˆå¤šé£æ ¼å¤‡é€‰ï¼‰\n"
          ].join("")
        : ""

      return `${prefix}ğŸŒŸ **è¨€å¯ä¸‡è±¡ - AIåˆ›æ„å¹³å°** æ¬¢è¿æ‚¨ï¼

ğŸ­ **åˆ›æ„å·¥åŠå…¨è§ˆï¼š**

ğŸ“ **æ–‡åˆ›å·¥åŠ** - æ™ºèƒ½æ–‡æ¡ˆåˆ›ä½œ
â€¢ è¥é”€æ–‡æ¡ˆã€äº§å“æè¿°ã€æ•…äº‹åˆ›ä½œ
â€¢ è¯—æ­Œç”Ÿæˆã€å‰§æœ¬åˆ›ä½œã€æ–°é—»ç¨¿ä»¶
${creativeExamples}
ğŸ¨ **å³æ¢¦AI** - è§†è§‰åˆ›æ„ç”Ÿæˆ  
â€¢ å›¾åƒç”Ÿæˆã€é£æ ¼è½¬æ¢ã€logoè®¾è®¡
â€¢ æµ·æŠ¥åˆ¶ä½œã€æ’ç”»åˆ›ä½œã€æ¦‚å¿µè®¾è®¡

ğŸ¬ **å½±åƒåˆ›ä½œ** - è§†é¢‘åŠ¨ç”»åˆ¶ä½œ
â€¢ è§†é¢‘å‰ªè¾‘ã€åŠ¨ç”»ç”Ÿæˆã€ç‰¹æ•ˆåˆ¶ä½œ
â€¢ é…éŸ³åˆæˆã€å­—å¹•ç”Ÿæˆã€çŸ­è§†é¢‘åˆ›ä½œ

ğŸ’« **ä½¿ç”¨æ–¹å¼ï¼š**
â€¢ ç›´æ¥æè¿°æ‚¨çš„åˆ›æ„éœ€æ±‚
â€¢ ä¸Šä¼ å‚è€ƒå›¾ç‰‡æˆ–æ–‡æ¡£
â€¢ æŒ‡å®šé£æ ¼ã€è‰²è°ƒã€æƒ…æ„ŸåŸºè°ƒ
â€¢ æˆ‘ä¼šä¸ºæ‚¨æä¾›ä¸“ä¸šçš„åˆ›æ„æ–¹æ¡ˆ

å‡†å¤‡å¥½é‡Šæ”¾æ‚¨çš„åˆ›æ„äº†å—ï¼Ÿå‘Šè¯‰æˆ‘æ‚¨çš„æƒ³æ³•ï¼`
    }

    // ç³»ç»ŸçŠ¶æ€æŸ¥è¯¢
    if (["çŠ¶æ€", "ç³»ç»Ÿ", "è¿è¡Œ"].some((keyword) => lowerInput.includes(keyword))) {
      // éšæœºæ¨¡å¼ï¼šæ·»åŠ æ›´å¤šç³»ç»Ÿç»†èŠ‚ï¼›ééšæœºæ¨¡å¼ï¼šä¿æŒåŸºç¡€çŠ¶æ€
      const systemDetails = isRandomMode
        ? `â€¢ å†…å­˜å ç”¨ï¼š${Math.floor(Math.random() * 30 + 40)}%
â€¢ ç£ç›˜ç©ºé—´ï¼š${Math.floor(Math.random() * 20 + 70)}GB å¯ç”¨
`
        : ""

      return `${prefix}ğŸ–¥ï¸ **NEXUS OS ç³»ç»ŸçŠ¶æ€æŠ¥å‘Š**

ğŸŸ¢ **æ ¸å¿ƒæœåŠ¡çŠ¶æ€ï¼š**
â€¢ AI å¼•æ“ï¼šåœ¨çº¿è¿è¡Œ (${modelId})
â€¢ æœ¬åœ°æ¨¡å‹ï¼š${localModelDiscovery.getDiscoveredModels().length} ä¸ªå·²å‘ç°
â€¢ åˆ›æ„å¼•æ“ï¼šé«˜æ€§èƒ½æ¨¡å¼
â€¢ æ•°æ®åº“ï¼šè¿æ¥æ­£å¸¸  
â€¢ ç½‘ç»œæœåŠ¡ï¼šç¨³å®š
${systemDetails}
ğŸ“Š **æ¨¡å‹çŠ¶æ€ï¼š**
â€¢ å½“å‰æ¨¡å‹ï¼š${modelId}
â€¢ å“åº”å»¶è¿Ÿï¼š${Math.floor(Math.random() * 200 + 50)}ms
â€¢ å¯ç”¨æ€§ï¼š99.9%

ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½ï¼Œæ‰€æœ‰åŠŸèƒ½æ­£å¸¸å¯ç”¨ï¼`
    }

    // é»˜è®¤å›å¤
    return `${prefix}æˆ‘æ­£åœ¨ä½¿ç”¨ **${modelId}** æ¨¡å‹ä¸ºæ‚¨æœåŠ¡ã€‚

ğŸ¯ **æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š**
â€¢ åˆ›æ„å†…å®¹ç”Ÿæˆ - æ–‡æ¡ˆã€å›¾åƒã€è§†é¢‘
â€¢ æ•°æ®åˆ†æå¤„ç† - æŠ¥è¡¨ã€ç»Ÿè®¡ã€å¯è§†åŒ–
â€¢ ç³»ç»ŸåŠŸèƒ½æ“ä½œ - å®¢æˆ·ç®¡ç†ã€è¡¨å•åˆ›å»º
â€¢ æ™ºèƒ½å¯¹è¯äº¤äº’ - é—®ç­”ã€å»ºè®®ã€æŒ‡å¯¼

ğŸ’¡ **ä½¿ç”¨æç¤ºï¼š**
â€¢ è¯¦ç»†æè¿°æ‚¨çš„éœ€æ±‚è·å¾—æ›´å¥½çš„å›å¤
â€¢ å°è¯•è¯´"åˆ›æ„"ä½“éªŒAIå†…å®¹ç”Ÿæˆ
â€¢ è¯´"ç³»ç»ŸçŠ¶æ€"æŸ¥çœ‹è¿è¡Œæƒ…å†µ
â€¢ ä¸Šä¼ æ–‡ä»¶è¿›è¡Œæ‰¹é‡å¤„ç†

æœ‰ä»€ä¹ˆå…·ä½“éœ€è¦å¸®åŠ©çš„å—ï¼Ÿ`
  }

  // ä¼°ç®—tokenæ•°é‡ï¼ˆç®€å•å®ç°ï¼‰
  private static estimateTokens(text: string): number {
    // ä¸­æ–‡æŒ‰å­—ç¬¦è®¡ç®—ï¼Œè‹±æ–‡æŒ‰å•è¯è®¡ç®—
    const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length
    const englishWords = text
      .replace(/[\u4e00-\u9fff]/g, "")
      .split(/\s+/)
      .filter((w) => w.length > 0).length

    return Math.ceil(chineseChars * 1.5 + englishWords)
  }

  // è·å–æ¨¡å‹ä¿¡æ¯
  static async getModelInfo(modelId: string): Promise<{id: string; name: string; provider: string; type: 'local' | 'cloud'; status: string; capabilities: string[]; endpoint?: string} | null> {
    const localModels = localModelDiscovery.getDiscoveredModels()
    const localModel = localModels.find((m) => m.id === modelId)

    if (localModel) {
      return {
        id: localModel.id,
        name: localModel.name,
        provider: localModel.provider,
        type: "local" as const,
        status: localModel.status,
        capabilities: localModel.capabilities,
        endpoint: localModel.endpoint,
      }
    }

    // äº‘ç«¯æ¨¡å‹ä¿¡æ¯
    const cloudModels = {
      "gpt-4": { name: "GPT-4", provider: "OpenAI", capabilities: ["chat", "reasoning", "code"] },
      "gpt-3.5": { name: "GPT-3.5", provider: "OpenAI", capabilities: ["chat", "completion"] },
      "claude-3": { name: "Claude-3", provider: "Anthropic", capabilities: ["chat", "analysis", "writing"] },
      "gemini-pro": { name: "Gemini Pro", provider: "Google", capabilities: ["chat", "vision", "multimodal"] },
      "jimeng-ai": { name: "å³æ¢¦AI", provider: "JiMeng", capabilities: ["creative", "image", "design"] },
    }

    const cloudModel = cloudModels[modelId as keyof typeof cloudModels]
    if (cloudModel) {
      return {
        id: modelId,
        name: cloudModel.name,
        provider: cloudModel.provider,
        type: "cloud" as const,
        status: "online" as const,
        capabilities: cloudModel.capabilities,
      }
    }

    return null
  }
}